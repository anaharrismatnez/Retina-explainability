{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from utils import nethook, imgviz, show,tally\n",
    "\n",
    "\n",
    "#https://github.com/SIDN-IAP/global-model-repr\n",
    "#https://github.com/davidbau/dissect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running pytorch', torch.__version__, 'using', device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile=0.01\n",
    "percent_level = 1.0 - quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from torchvision import models\n",
    "\n",
    "def get_model(device,base_model=None):\n",
    "    '''\n",
    "       Gets VGG16 model\n",
    "       :param base_model: path to pre initialized model\n",
    "       :return: vgg16 model with last layer modification (2 classes)\n",
    "       '''\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    #print(models.vgg16(pretrained=True))\n",
    "\n",
    "    # Freeze trained weights\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Newly created modules have require_grad=True by default \n",
    "    num_features = model.classifier[6].in_features\n",
    "    features = list(model.classifier.children())[:-1]  # Remove last layer\n",
    "    linear = nn.Linear(num_features, 2)\n",
    "\n",
    "    features.extend([linear])  # Add our layer with 2 outputs\n",
    "    model.classifier = nn.Sequential(*features)  # Replace the model classifier\n",
    "    #print(model)\n",
    "    # Load pre initialized model\n",
    "    if base_model and os.path.exists(base_model):\n",
    "        model.load_state_dict(torch.load(base_model, map_location=device))\n",
    "        logging.info(f'Loading {base_model}')\n",
    "    else:\n",
    "        logging.info(f'Loading pretrained VGG16 model')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,img_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_tensor = img_tensor.cuda()\n",
    "        output = model(img_tensor)\n",
    "        score, predicted = torch.max(output.data, 1)\n",
    "        out = [predicted.item(),score.item()]\n",
    "    return out\n",
    "\n",
    "#print(eval(model,test_data_loader[0][0][None])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class CustomImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        super(CustomImageFolder, self).__init__(dataset, transform=transform)\n",
    "\n",
    "    def _find_classes(self, dir: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        classes.sort()\n",
    "        classes = [item for item in reversed(classes)]\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample, label = super(datasets.ImageFolder, self).__getitem__(index)\n",
    "        return sample, label, self.imgs[index]\n",
    "\n",
    "\n",
    "def load_and_transform_data(dataset, batch_size=1, data_augmentation=False):\n",
    "    # Define transformations that will be applied to the images\n",
    "    # VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "    logging.info(f'Loading data from {dataset}')\n",
    "    \n",
    "    mean=[0.48, 0.24, 0.12]\n",
    "    std=[0.27, 0.14, 0.08]\n",
    "  \n",
    "\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4387, 0.3090, 0.2211], std=[0.2733, 0.2035, 0.1717]),\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageFolder(dataset, transform=data_transforms)\n",
    "    #data_loader = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    logging.info(f'Loaded {len(dataset)} images under {dataset}: Classes: {dataset.class_to_idx}')\n",
    "\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model('/home/aharris/shared/EyePACS/models/exp9/weights_190.pth')\n",
    "model.to(device=device)\n",
    "\n",
    "test_image_folder = '/home/aharris/shared/EyePACS/input/image/dynamic_run/test'\n",
    "\n",
    "test_data_loader = load_and_transform_data(test_image_folder)\n",
    "\n",
    "#renorm = renormalize.renormalizer(source=test_data_loader, target='zc')\n",
    "ivsmall = imgviz.ImageVisualizer((56, 56), source=test_data_loader, percent_level=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv = imgviz.ImageVisualizer(224, image_size= (224,224),source=test_data_loader, percent_level=0.99)\n",
    "show(iv.image(test_data_loader[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine raw unit activations.\n",
    "\n",
    "Look at individual activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layername = 'features.28'\n",
    "model = nethook.InstrumentedModel(model)\n",
    "model.retain_layer(layername)\n",
    "indexes = range(0, 263)\n",
    "batch = torch.stack([test_data_loader[i][0] for i in indexes])\n",
    "model(batch.cuda())\n",
    "acts = model.retained_layer(layername).cpu()\n",
    "show([\n",
    "    [\n",
    "        [ivsmall.masked_image(batch[imagenum], acts[imagenum], unitnum)],\n",
    "        [ivsmall.heatmap(acts[imagenum], unitnum, mode='nearest')],\n",
    "        'unit %d' % unitnum\n",
    "    ]\n",
    "    for unitnum in range(acts.shape[1])\n",
    "    for imagenum in [22]\n",
    "])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine images that maximize each unit\n",
    "The loop below identifies the images, out of a sample of 30, that cause each filter to activate strongest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 263\n",
    "def max_activations(batch, *args):\n",
    "    image_batch = batch.to(device)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    return acts.view(acts.shape[:2] + (-1,)).max(2)[0]\n",
    "\n",
    "def mean_activations(batch, *args):\n",
    "    image_batch = batch.to(device)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    return acts.view(acts.shape[:2] + (-1,)).mean(2)\n",
    "\n",
    "topk = tally.tally_topk(\n",
    "    max_activations,\n",
    "    dataset=test_data_loader,\n",
    "    sample_size=sample_size,\n",
    "    cachefile='results/cache_mean_topk.npz'\n",
    ")\n",
    "\n",
    "top_scores = topk.result()[0]\n",
    "top_indexes =topk.result()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_array = np.ndarray([512,30],dtype=object)\n",
    "for i in range(512):\n",
    "    for j in range(30):\n",
    "        top_array[i,j] = [top_indexes[i][j].item(),top_scores[i][j].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop that runs the model for each of the top-activating images for a particular unit (12), and then shows where that unit activates within the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iv = imgviz.ImageVisualizer(224, image_size= (224,224),source=test_data_loader, level=rq.quantiles(percent_level),quantiles=rq)\n",
    "iv = imgviz.ImageVisualizer(224, image_size= (224,224),source=test_data_loader)\n",
    "#iv = imgviz.ImageVisualizer(224, image_size= (224,224),source=test_data_loader, percent_level=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = '/home/aharris/shared/EyePACS/interpretability/results_dissection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in range(1,512):\n",
    "    img = show.blocks([\n",
    "        ['unit %d' % u,\n",
    "        'img %d' % top_array[u][j][0],\n",
    "        'pred: %s' % test_data_loader.classes[eval(model,test_data_loader[top_array[u][j][0]][0][None])[0]],\n",
    "        'score: %f' %top_array[u][j][1],\n",
    "        'GroundTruth: %s' %str(test_data_loader[top_array[u][j][0]][2]).split('/')[3],\n",
    "        [iv.masked_image(\n",
    "            test_data_loader[top_array[u][j][0]][0],\n",
    "            model.retained_layer(layername)[0],\n",
    "            u)]\n",
    "        ]\n",
    "        for j in range(30)\n",
    "        ])\n",
    "\n",
    "\n",
    "    html = img.data\n",
    "    with open(f'{out_path}/{u}_unit_top30.html', 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
